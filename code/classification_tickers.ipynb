{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c4b958",
   "metadata": {},
   "source": [
    "## Classification of stock tickers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0d34688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import io\n",
    "import xlrd \n",
    "import pandas as pd\n",
    "import random as rd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# cd drive/'My Drive'/'Colab Notebooks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0be8e480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Name fund_ticker Percent Pronounceable deg_level  \\\n",
      "0           Fidelity® Contrafund®       FCNTX                22.70%        MA   \n",
      "1  T. Rowe Price Blue Chip Growth       TRBCX                21.67%        MA   \n",
      "2      T. Rowe Price Growth Stock       PRGFX                23.68%        MA   \n",
      "3        Fidelity® Growth Company       FDGRX                24.14%        MA   \n",
      "4    Vanguard Dividend Growth Inv       VDIGX                17.39%        MA   \n",
      "\n",
      "  deg_subject  CFA   exp         ManagerName      category          size  \\\n",
      "0     Finance   No  32.0      William Danoff  Large Growth  1.353730e+11   \n",
      "1     Finance  Yes  27.0     Larry J. Puglia  Large Growth  5.832290e+10   \n",
      "2     Finance   No  18.0      Joseph B. Fath  Large Growth  5.612382e+10   \n",
      "3     Finance   No  29.0     Steven S. Wymer  Large Growth  4.827930e+10   \n",
      "4     Finance   No  22.0  Donald J. Kilbride   Large Blend  3.434094e+10   \n",
      "\n",
      "   TotalRetRankCat3Ymoendm ms_rating     esg_rating  \\\n",
      "0                     28.0   4 Stars  Below Average   \n",
      "1                     11.0   4 Stars            Low   \n",
      "2                     32.0   3 Stars            Low   \n",
      "3                      4.0   5 Stars  Below Average   \n",
      "4                     54.0   3 Stars  Above Average   \n",
      "\n",
      "  MorningstarAnalystRatingdaye MorningstarCategoryPrimaryBenc     ret_3y  \\\n",
      "0                       Silver                 S&P 500 TR USD  12.691394   \n",
      "1                         Gold                 S&P 500 TR USD  14.183781   \n",
      "2                       Bronze                 S&P 500 TR USD  12.361444   \n",
      "3                       Silver                 S&P 500 TR USD  15.779205   \n",
      "4                         Gold                 S&P 500 TR USD  10.021960   \n",
      "\n",
      "   alpha_3y  beta_3y  \n",
      "0     0.150    1.109  \n",
      "1     0.965    1.171  \n",
      "2    -0.228    1.125  \n",
      "3     0.021    1.428  \n",
      "4     0.562    0.804  \n"
     ]
    }
   ],
   "source": [
    "# Import data from github;\n",
    "url = \"https://github.com/clairepaoli/NLP_stock_tickers/blob/main/data/stata_data.csv?raw=true\" \n",
    "download = requests.get(url).content\n",
    "\n",
    "df = pd.read_csv(io.StringIO(download.decode('utf-8')))\n",
    "print (df.head())\n",
    "\n",
    "# Alternatively, save locally and import;\n",
    "# df = pd.read_excel(\".../data/stata_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "571286cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary variables;\n",
    "funds = list(df[\"fund_ticker\"])\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cce4ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pronounceable share;\n",
    "def pct_pron(list):\n",
    "    count = 0\n",
    "    for i in list:\n",
    "        if i == \"pronounceable\":\n",
    "            count += 1\n",
    "    return count/len(list)\n",
    "\n",
    "def scramble(s):\n",
    "    return \"\".join(rd.sample(s, len(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251a1a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a73a149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'a',\n",
       " 'aa',\n",
       " 'aal',\n",
       " 'aalii',\n",
       " 'aam',\n",
       " 'Aani',\n",
       " 'aardvark',\n",
       " 'aardwolf',\n",
       " 'Aaron']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "word_list = words.words()\n",
    "\n",
    "word_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4846cbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'aa',\n",
       " 'aal',\n",
       " 'aalii',\n",
       " 'aam',\n",
       " 'aardvark',\n",
       " 'aardwolf',\n",
       " 'aba',\n",
       " 'abac',\n",
       " 'abaca']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pron = [word.strip() for word in word_list if word == word.lower()]\n",
    "pron[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d248f7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'aa',\n",
       " 'laa',\n",
       " 'liaia',\n",
       " 'ama',\n",
       " 'rkaraadv',\n",
       " 'orflawad',\n",
       " 'aba',\n",
       " 'abca',\n",
       " 'aacab']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create set of pronounceable and non-pronounceable;\n",
    "\n",
    "# unpron = [scramble(word) for word in pron]\n",
    "import random\n",
    "unpron = [''.join(random.sample(word, len(word))) for word in pron]\n",
    "\n",
    "unpron[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8d151c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pron</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aalii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pron\n",
       "0      a\n",
       "1     aa\n",
       "2    aal\n",
       "3  aalii\n",
       "4    aam"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pron = pd.DataFrame (pron, columns = ['pron'])\n",
    "#unpron = pd.DataFrame (unpron, columns = ['unpron'])\n",
    "\n",
    "#pron.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5cf455",
   "metadata": {},
   "source": [
    "We then use the **train_test_split** function to randomly split our data. The first argument will be the feature data, the second the targets or labels. The test_size keyword argument specifies what proportion of the original data is used for the test set. Lastly, the random_state sets a seed for the random number generator that splits the data into train and test. Setting the seed with the same argument later will allow you to reproduce the exact split and your downstream results. train test split returns four arrays: the training data, the test data, the training labels, and the test labels.\n",
    "\n",
    "We specify the size of the test to 30%.\n",
    "\n",
    "It is also best practice to perform your split so that the split reflects the labels on your data. That is, you want the labels to be distributed in train and test sets as they are in the original dataset. To achieve this, we use the keyword argument stratify = y, where y the list or array containing the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d380894",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pron + unpron\n",
    "y = ['pronounceable']*len(pron) + ['unpronounceable']*len(unpron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec368ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data between train and test set;\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 21, stratify = y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bda3c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296150"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76584432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126922"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9940e52a",
   "metadata": {},
   "source": [
    "In order to use textual data for predictive modeling, the text must be parsed to remove certain words – this process is called **tokenization**. These words need to then be encoded as integers, or floating-point values, for use as inputs in machine learning algorithms. This process is called feature extraction (or vectorization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f90508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'john': 4, 'is': 3, 'good': 2, 'boy': 1, 'watches': 5, 'basketball': 0}\n",
      "(1, 6)\n",
      "[[1 1 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "# Example of using CountVectorizer;\n",
    "\n",
    "# list of text documents\n",
    "text = [\"John is a good boy. John watches basketball\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a460cf2",
   "metadata": {},
   "source": [
    "We use the MultinomialNB classifier, a Naive Bayes classifier for multinomial models. The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc57d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build classifier using bigrams and trigrams of words in dictionary\n",
    "classify = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char', ngram_range=(1, 3))),\n",
    "    ('clf', MultinomialNB())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32604e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and test the algorithm;\n",
    "classify = classify.fit(X_train, y_train)\n",
    "y_pred = classify.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9c41547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.923015710436331\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  pronounceable       0.91      0.93      0.92     63461\n",
      "unpronounceable       0.93      0.91      0.92     63461\n",
      "\n",
      "       accuracy                           0.92    126922\n",
      "      macro avg       0.92      0.92      0.92    126922\n",
      "   weighted avg       0.92      0.92      0.92    126922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print test results\n",
    "print('Accuracy Score:', metrics.accuracy_score(y_test, y_pred))\n",
    "print('Classification Report:')\n",
    "print(metrics.classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba6b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in each set of holdings data and apply algorithm to each of the funds' holdings;\n",
    "output_file = open(\".../result.csv\", \"w\")\n",
    "matching = []\n",
    "\n",
    "for fund in funds:\n",
    "    try:\n",
    "        fund_df = pd.read_excel(\"/Users/jake/Desktop/Fine 547 Holdings/\" + fund + \".xlsx\")\n",
    "        total += 1\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    fund_ticks = list(fund_df[\"Unnamed: 1\"][5:])\n",
    "    est_pron = classify.predict(fund_ticks)\n",
    "    \n",
    "    output = ['NaN', 'NaN', 'NaN', 'Pronouncable', pct_pron(est_pron)] + [x for x in est_pron]\n",
    "    \n",
    "    fund_df['pronounceable'] = output\n",
    "    \n",
    "    matching.append((fund, pct_pron(est_pron)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591edfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(matching)\n",
    "# Write results into a new the output file\n",
    "for (f, v) in matching:\n",
    "    output_file.write(str(f) + \",\" + str(v))\n",
    "    output_file.write(\"\\n\")\n",
    "output_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
